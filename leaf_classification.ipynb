{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "leaf-classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vuwXKzMqlqT"
      },
      "source": [
        "import os\n",
        "import tarfile\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LRY3WeMl0c5",
        "outputId": "d67974ad-7fcc-40a5-a1c2-536ce0b6ba55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "http://leafsnap.com/static/dataset/leafsnap-dataset.tar \\\n",
        "    -O /tmp/dataset.tar"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-21 20:51:21--  http://leafsnap.com/static/dataset/leafsnap-dataset.tar\n",
            "Resolving leafsnap.com (leafsnap.com)... 128.59.23.133\n",
            "Connecting to leafsnap.com (leafsnap.com)|128.59.23.133|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1023416320 (976M) [application/x-tar]\n",
            "Saving to: ‘/tmp/dataset.tar’\n",
            "\n",
            "/tmp/dataset.tar    100%[===================>] 976.01M  89.0MB/s    in 14s     \n",
            "\n",
            "2020-05-21 20:51:35 (70.2 MB/s) - ‘/tmp/dataset.tar’ saved [1023416320/1023416320]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2RMMfHZtAuv"
      },
      "source": [
        "local_zip = '/tmp/dataset.tar'\n",
        "zip_ref   = tarfile.open(local_zip, 'r')\n",
        "zip_ref.extractall('/content/tmp')\n",
        "zip_ref.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfDJ9GRv2_t9",
        "outputId": "1d99cd7c-168b-449d-bd4c-5947b5b2bd7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "data_dir=\"/content/tmp/dataset/images/field\"\n",
        "train_dir='/content/tmp/train/'\n",
        "test_dir='/content/tmp/test/'\n",
        "data_lis = os.listdir(data_dir)\n",
        "print(data_lis)\n",
        "try:\n",
        "  if(not os.path.exists('/content/tmp/train')):\n",
        "    os.mkdir('/content/tmp/'+'train')\n",
        "  if(not os.path.exists('/content/tmp/test')):\n",
        "    os.mkdir('/content/tmp/'+'test')\n",
        "  for leaf_name in data_lis:\n",
        "    if(not os.path.exists('/content/tmp/train/'+leaf_name)):\n",
        "      os.mkdir('/content/tmp/train/'+leaf_name)\n",
        "    if(not os.path.exists('/content/tmp/test/'+leaf_name)):\n",
        "      os.mkdir('/content/tmp/test/'+leaf_name)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['syringa_reticulata', 'ostrya_virginiana', 'malus_baccata', 'catalpa_bignonioides', 'crataegus_phaenopyrum', 'morus_alba', 'quercus_phellos', 'styrax_obassia', 'pinus_sylvestris', 'corylus_colurna', 'ulmus_glabra', 'magnolia_denudata', 'populus_deltoides', 'quercus_montana', 'staphylea_trifolia', 'ulmus_americana', 'crataegus_pruinosa', 'malus_hupehensis', 'asimina_triloba', 'gleditsia_triacanthos', 'ailanthus_altissima', 'cercis_canadensis', 'prunus_serrulata', 'abies_concolor', 'ulmus_parvifolia', 'acer_saccharum', 'ficus_carica', 'crataegus_crus-galli', 'chionanthus_retusus', 'acer_pseudoplatanus', 'pinus_wallichiana', 'pinus_thunbergii', 'quercus_shumardii', 'aesculus_pavi', 'castanea_dentata', 'amelanchier_laevis', 'abies_nordmanniana', 'chamaecyparis_pisifera', 'pseudolarix_amabilis', 'catalpa_speciosa', 'taxodium_distichum', 'acer_griseum', 'fraxinus_americana', 'tilia_cordata', 'eucommia_ulmoides', 'populus_tremuloides', 'gymnocladus_dioicus', 'quercus_coccinea', 'zelkova_serrata', 'pyrus_calleryana', 'juniperus_virginiana', 'quercus_palustris', 'tilia_europaea', 'ginkgo_biloba', 'quercus_virginiana', 'prunus_sargentii', 'sassafras_albidum', 'pinus_cembra', 'cedrus_libani', 'quercus_velutina', 'quercus_acutissima', 'tilia_tomentosa', 'aesculus_hippocastamon', 'ilex_opaca', 'prunus_serotina', 'cladrastis_lutea', 'acer_platanoides', 'pinus_virginiana', 'cornus_kousa', 'quercus_marilandica', 'liriodendron_tulipifera', 'quercus_rubra', 'malus_coronaria', 'populus_grandidentata', 'acer_pensylvanicum', 'pinus_pungens', 'malus_floribunda', 'crataegus_viridis', 'pinus_koraiensis', 'styrax_japonica', 'malus_pumila', 'crataegus_laevigata', 'cedrus_deodara', 'cryptomeria_japonica', 'larix_decidua', 'acer_palmatum', 'carya_tomentosa', 'quercus_imbricaria', 'acer_ginnala', 'metasequoia_glyptostroboides', 'magnolia_acuminata', 'tsuga_canadensis', 'carya_cordiformis', 'celtis_tenuifolia', 'quercus_bicolor', 'acer_saccharinum', 'quercus_nigra', 'cercidiphyllum_japonicum', 'prunus_pensylvanica', 'magnolia_stellata', 'malus_angustifolia', 'chionanthus_virginicus', 'celtis_occidentalis', 'salix_caroliniana', 'cornus_florida', 'betula_jacqemontii', 'magnolia_soulangiana', 'betula_nigra', 'fagus_grandifolia', 'quercus_muehlenbergii', 'robinia_pseudo-acacia', 'betula_alleghaniensis', 'albizia_julibrissin', 'carya_glabra', 'acer_rubrum', 'toona_sinensis', 'evodia_daniellii', 'magnolia_macrophylla', 'pinus_taeda', 'betula_lenta', 'acer_campestre', 'carpinus_caroliniana', 'morus_rubra', 'pinus_flexilis', 'pinus_rigida', 'carpinus_betulus', 'broussonettia_papyrifera', 'prunus_yedoensis', 'prunus_virginiana', 'nyssa_sylvatica', 'cedrus_atlantica', 'juglans_nigra', 'fraxinus_pennsylvanica', 'maclura_pomifera', 'cornus_mas', 'quercus_alba', 'magnolia_virginiana', 'pinus_densiflora', 'pinus_resinosa', 'acer_negundo', 'chamaecyparis_thyoides', 'salix_babylonica', 'quercus_macrocarpa', 'platanus_occidentalis', 'quercus_robur', 'quercus_michauxii', 'pinus_parviflora', 'ulmus_rubra', 'stewartia_pseudocamellia', 'betula_populifolia', 'halesia_tetraptera', 'magnolia_grandiflora', 'salix_matsudana', 'juglans_cinerea', 'amelanchier_arborea', 'tilia_americana', 'quercus_stellata', 'fraxinus_nigra', 'aesculus_flava', 'aesculus_glabra', 'quercus_cerris', 'pinus_bungeana', 'pinus_peucea', 'pinus_strobus', 'pinus_echinata', 'prunus_subhirtella', 'paulownia_tomentosa', 'koelreuteria_paniculata', 'carya_ovata', 'ulmus_pumila', 'platanus_acerifolia', 'ptelea_trifoliata', 'salix_nigra', 'picea_abies', 'oxydendrum_arboreum', 'pinus_nigra', 'amelanchier_canadensis', 'liquidambar_styraciflua', 'picea_orientalis', 'picea_pungens', 'magnolia_tripetala', 'quercus_falcata', 'diospyros_virginiana', 'phellodendron_amurense']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU54J-R8uH7d"
      },
      "source": [
        "import glob\n",
        "for fold in data_lis:\n",
        "  fn=glob.glob('/content/tmp/test/'+fold+'/*.jpg') #gives a list of files\n",
        "  for i in fn:\n",
        "      if os.path.exists(i):\n",
        "        os.remove(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xKuhrdxClMD"
      },
      "source": [
        "import numpy as np\n",
        "def split_data(SOURCE,train_dir,test_dir,split_size):\n",
        "  files = []\n",
        "  for filename in os.listdir(SOURCE):\n",
        "    file = SOURCE + filename\n",
        "    if os.path.getsize(file) > 0:\n",
        "      files.append(filename)\n",
        "    else:\n",
        "      print(filename + \" is zero length, so ignoring.\")\n",
        "  training_length = int(len(files) * split_size)\n",
        "  testing_length = int(len(files) - training_length)\n",
        "  shuffled_set = random.sample(files, len(files))\n",
        "  training_set = shuffled_set[0:training_length]\n",
        "  testing_set = shuffled_set[-testing_length:]\n",
        "  h1=224\n",
        "  w1=224\n",
        "  dimension=(w1,h1)\n",
        "  for filename in training_set:\n",
        "    this_file = str(SOURCE + filename)\n",
        "    destination = str(train_dir + filename)\n",
        "    img=cv2.imread(this_file)\n",
        "    #kernel = np.ones((5,5),np.float32)/25 #4x4 window size and the valuse with that windowa are summed up and diveide by(16) which is 4*4. \n",
        "    #dst = cv2.filter2D(this_file,-1,kernel)\n",
        "    re_img=cv2.resize(img,dimension,interpolation=cv2.INTER_AREA)\n",
        "    cv2.imwrite(this_file,re_img)\n",
        "    copyfile(this_file, destination)\n",
        "  for filename in testing_set:\n",
        "    this_file =str(SOURCE+filename)\n",
        "    destination = str(test_dir+filename)\n",
        "    img=cv2.imread(this_file)\n",
        "    #kernel = np.ones((5,5),np.float32)/25 #4x4 window size and the valuse with that windowa are summed up and diveide by(16) which is 4*4. \n",
        "    #dst = cv2.filter2D(this_file,-1,kernel)\n",
        "    re_img=cv2.resize(img,dimension,interpolation=cv2.INTER_AREA)\n",
        "    cv2.imwrite(this_file,re_img)\n",
        "    copyfile(this_file,destination)\n",
        "#for each file in RGB directory\n",
        "source_dir=\"/content/tmp/dataset/images/field/\"\n",
        "train_dir='/content/tmp/train/'\n",
        "test_dir='/content/tmp/test/'\n",
        "data_lis = os.listdir(data_dir)\n",
        "for dir_name in data_lis:\n",
        "  SOURCE = source_dir+dir_name+\"/\"\n",
        "  TRAINING = train_dir+dir_name+\"/\"\n",
        "  TESTING = test_dir+dir_name+\"/\"\n",
        "  split_data(SOURCE,TRAINING,TESTING,0.99)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1nViVT-zluI"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('acc')>0.999):\n",
        "      print(\"\\nReached 99.9% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh7CaAHkF8Nr"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    rotation_range=45,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baXiYHmQWBTv",
        "outputId": "cbc3b887-da20-4cf3-8cd9-b4dc43de1417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, Input\n",
        "from keras.models import Model\n",
        "model = VGG16(input_tensor=Input(shape=(224, 224, 3)),weights='imagenet', include_top=False)\n",
        "\n",
        "\n",
        "x=Flatten()(model.output)\n",
        "x = Dense(4096,activation = 'relu')(x)\n",
        "x=Dense(184, activation='softmax')(x)\n",
        "model=Model(model.input,x)\n",
        "#for layer in model.layers:\n",
        "  #layer.trainable = False\n",
        "model.summary()\n",
        "print(len(model.layers))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 184)               753848    \n",
            "=================================================================\n",
            "Total params: 118,233,080\n",
            "Trainable params: 118,233,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-KtYRMSKgiH"
      },
      "source": [
        "\n",
        "'''model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(50, 50, 3)),\n",
        "\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  \n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    \n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(40, activation='softmax')\n",
        "])'''\n",
        "import keras\n",
        "model.compile(optimizer = keras.optimizers.Adam(),\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcltMW12L1CN",
        "outputId": "a3d8eb3e-c16a-4d90-996f-1a38d3d3bc76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "TRAINING_DIR = \"/content/tmp/train/\"\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size=32,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(224, 224))\n",
        "VALIDATION_DIR = \"/content/tmp/test/\"\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              batch_size=32,\n",
        "                                                              class_mode='categorical',\n",
        "                                                              target_size=(224, 224))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 7528 images belonging to 184 classes.\n",
            "Found 191 images belonging to 184 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivjNZBeNu2jW",
        "outputId": "2e518436-f439-4811-e626-650ca1bf1b45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(len(acc))\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.axis([0,25, 0, 1])\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.axis([0,25, 0, 1])\n",
        "plt.figure()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f9ddb4a40b42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLjjv7_tt70u",
        "outputId": "ad2f5b98-d58c-4d8b-95ab-cc648c1fe165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "#model.summary()\n",
        "#model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit_generator(train_generator, epochs=50 ,validation_data = validation_generator)\n",
        "\n",
        "model.save(\"leafdata.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "236/236 [==============================] - 84s 354ms/step - loss: 5.3467 - acc: 0.0221 - val_loss: 5.3078 - val_acc: 0.0105\n",
            "Epoch 2/50\n",
            "236/236 [==============================] - 76s 322ms/step - loss: 5.0817 - acc: 0.0221 - val_loss: 5.1063 - val_acc: 0.0105\n",
            "Epoch 3/50\n",
            "236/236 [==============================] - 77s 327ms/step - loss: 5.0751 - acc: 0.0240 - val_loss: 5.1642 - val_acc: 0.0105\n",
            "Epoch 4/50\n",
            "236/236 [==============================] - 77s 327ms/step - loss: 5.0748 - acc: 0.0228 - val_loss: 5.2210 - val_acc: 0.0105\n",
            "Epoch 5/50\n",
            "236/236 [==============================] - 77s 325ms/step - loss: 5.0716 - acc: 0.0239 - val_loss: 5.3056 - val_acc: 0.0105\n",
            "Epoch 6/50\n",
            "236/236 [==============================] - 77s 326ms/step - loss: 5.0716 - acc: 0.0236 - val_loss: 5.3177 - val_acc: 0.0105\n",
            "Epoch 7/50\n",
            "236/236 [==============================] - 77s 326ms/step - loss: 5.0711 - acc: 0.0213 - val_loss: 5.1970 - val_acc: 0.0105\n",
            "Epoch 8/50\n",
            "236/236 [==============================] - 78s 329ms/step - loss: 5.0697 - acc: 0.0230 - val_loss: 5.2632 - val_acc: 0.0105\n",
            "Epoch 9/50\n",
            "236/236 [==============================] - 77s 327ms/step - loss: 5.0702 - acc: 0.0240 - val_loss: 5.3957 - val_acc: 0.0105\n",
            "Epoch 10/50\n",
            "214/236 [==========================>...] - ETA: 7s - loss: 5.0676 - acc: 0.0239"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2ZzlEZxEbkS"
      },
      "source": [
        "!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlp1_JVuEcO3",
        "outputId": "ac73a09d-5a6c-4ff0-90f0-3d8c4343760f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git init"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized empty Git repository in /content/.git/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBkHKfLXIWiG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a25tPtEdI0b_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2NJdn4B3oPj"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "#model=tf.keras.models.load_model('/content/rps.h5')\n",
        "path = '/content/tmp/train/liriodendron_tulipifera/12992002367158.jpg'\n",
        "img = image.load_img(path, target_size=(224, 224))\n",
        "y = tf.random.normal(shape=(224, 224, 3))\n",
        "x=tf.image.central_crop(y, 0.5)\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "images = np.vstack([x])\n",
        "classes = model.predict(images, batch_size=10)\n",
        "data_lis.sort()\n",
        "max_val=max(classes)\n",
        "print(classes)\n",
        "print(len(data_lis))\n",
        "data_lis.sort()\n",
        "print(data_lis[np.argmax(classes[0])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPtxPOzyIX7J",
        "outputId": "ac1ab595-26b3-4554-d07b-3c066c2a6a16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "!git commit -m \"first commit\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@59ee12f391ed.(none)')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1Ih8cGJIzJ_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}